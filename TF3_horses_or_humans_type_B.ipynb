{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF3-horses-or-humans-type-B",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danaing/TensorFlow-cert/blob/main/TF3_horses_or_humans_type_B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXmmcNxhB4Eb"
      },
      "source": [
        "#TF3-horsesorhumans-B-1-val-loss-0.1526"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpNdRzQH6Mo7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9642005-633a-4b97-d663-49a8549099f5"
      },
      "source": [
        "# Question\n",
        "#\n",
        "# This task requires you to create a classifier for horses or humans using\n",
        "# the provided data. Please make sure your final layer is a 1 neuron, activated by sigmoid as shown.\n",
        "# Please note that the test will use images that are 300x300 with 3 bytes color depth so be sure to design your neural network accordingly\n",
        "\n",
        "# =========== 합격 기준 가이드라인 공유 ============= #\n",
        "# val_loss 기준에 맞춰 주시는 것이 훨씬 더 중요 #\n",
        "# val_loss 보다 조금 높아도 상관없음. (언저리까지 OK) #\n",
        "# =================================================== #\n",
        "# 문제명: Category 3 - Horses Or Humans (Type B)\n",
        "# val_loss: 0.51 (더 낮아도 안 좋고, 높아도 안 좋음!)\n",
        "# val_acc: 관계없음\n",
        "# =================================================== #\n",
        "# =================================================== #\n",
        "\n",
        "\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import numpy as np\n",
        "from IPython.display import Image\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dropout, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "def solution_model():\n",
        "    _TRAIN_URL = \"https://storage.googleapis.com/download.tensorflow.org/data/horse-or-human.zip\"\n",
        "    _TEST_URL = \"https://storage.googleapis.com/download.tensorflow.org/data/validation-horse-or-human.zip\"\n",
        "    urllib.request.urlretrieve(_TRAIN_URL, 'horse-or-human.zip')\n",
        "    local_zip = 'horse-or-human.zip'\n",
        "    zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "    zip_ref.extractall('tmp/horse-or-human/')\n",
        "    zip_ref.close()\n",
        "    urllib.request.urlretrieve(_TEST_URL, 'validation-horse-or-human.zip')\n",
        "    local_zip = 'validation-horse-or-human.zip'\n",
        "    zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "    zip_ref.extractall('tmp/validation-horse-or-human/')\n",
        "    zip_ref.close()\n",
        "\n",
        "    TRAINING_DIR = 'tmp/horse-or-human/'\n",
        "    VALIDATION_DIR = 'tmp/validation-horse-or-human/'\n",
        "\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        #Your code here. Should at least have a rescale. Other parameters can help with overfitting.\n",
        "            rescale=1/ 255.0,\n",
        "            rotation_range=30,\n",
        "            width_shift_range=0.3,\n",
        "            height_shift_range=0.1,\n",
        "            shear_range=0.1,\n",
        "            zoom_range=0.3,\n",
        "            horizontal_flip=True,\n",
        "            fill_mode='nearest',\n",
        "        )\n",
        "\n",
        "    validation_datagen = ImageDataGenerator(\n",
        "        #Your Code here\n",
        "         rescale=1/ 255.0,)\n",
        "\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        #Your Code Here\n",
        "          TRAINING_DIR,\n",
        "          target_size=(300, 300),\n",
        "          batch_size=32,\n",
        "          class_mode='binary',\n",
        "          )\n",
        "\n",
        "    validation_generator = validation_datagen.flow_from_directory(\n",
        "        #Your Code Here\n",
        "          VALIDATION_DIR,\n",
        "          target_size=(300, 300),\n",
        "          batch_size=32,\n",
        "          class_mode='binary',\n",
        "          )\n",
        "\n",
        "\n",
        "    model = tf.keras.models.Sequential([\n",
        "        # Note the input shape specified on your first layer must be (300,300,3)\n",
        "        # Your Code here\n",
        "        Conv2D(16, (3, 3), activation='relu', input_shape=(300, 300, 3)),\n",
        "        MaxPooling2D(2, 2),\n",
        "        Conv2D(32, (3, 3), activation='relu'),\n",
        "        MaxPooling2D(2, 2),\n",
        "        Conv2D(64, (3, 3), activation='relu'),\n",
        "        MaxPooling2D(2, 2),\n",
        "        Conv2D(64, (3, 3), activation='relu'),\n",
        "        MaxPooling2D(2, 2),\n",
        "        Conv2D(128, (3, 3), activation='relu'),\n",
        "        MaxPooling2D(2, 2),\n",
        "        Flatten(),\n",
        "        Dropout(0.5),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dense(32, activation='relu'),\n",
        "        # This is the last layer. You should not change this code.\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "    checkpoint_path = \"tmp_checkpoint.ckpt\"\n",
        "    checkpoint = ModelCheckpoint(filepath=checkpoint_path, \n",
        "                                save_weights_only=True, \n",
        "                                save_best_only=True, \n",
        "                                monitor='val_loss', \n",
        "                                verbose=1)\n",
        "\n",
        "    model.fit(train_generator, \n",
        "              validation_data=(validation_generator),\n",
        "              epochs=25,\n",
        "              callbacks=[checkpoint],\n",
        "              )\n",
        "    model.load_weights(checkpoint_path)\n",
        "    # NOTE: If training is taking a very long time, you should consider setting the batch size appropriately on the generator, and the steps per epoch in the model.fit#\n",
        "    return model\n",
        "\n",
        "\n",
        "# Note that you'll need to save your model as a .h5 like this\n",
        "# This .h5 will be uploaded to the testing infrastructure\n",
        "# and a score will be returned to you\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"mymodel.h5\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1027 images belonging to 2 classes.\n",
            "Found 256 images belonging to 2 classes.\n",
            "Epoch 1/25\n",
            "33/33 [==============================] - 26s 800ms/step - loss: 0.6982 - acc: 0.4985 - val_loss: 0.6959 - val_acc: 0.5000\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.69595, saving model to tmp_checkpoint.ckpt\n",
            "Epoch 2/25\n",
            "33/33 [==============================] - 25s 770ms/step - loss: 0.6794 - acc: 0.5579 - val_loss: 0.8030 - val_acc: 0.5000\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.69595\n",
            "Epoch 3/25\n",
            "33/33 [==============================] - 25s 762ms/step - loss: 0.6021 - acc: 0.6670 - val_loss: 0.5712 - val_acc: 0.6094\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.69595 to 0.57118, saving model to tmp_checkpoint.ckpt\n",
            "Epoch 4/25\n",
            "33/33 [==============================] - 25s 761ms/step - loss: 0.4980 - acc: 0.7605 - val_loss: 2.2252 - val_acc: 0.5273\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.57118\n",
            "Epoch 5/25\n",
            "33/33 [==============================] - 25s 761ms/step - loss: 0.4457 - acc: 0.8130 - val_loss: 0.8452 - val_acc: 0.5078\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.57118\n",
            "Epoch 6/25\n",
            "33/33 [==============================] - 25s 763ms/step - loss: 0.3428 - acc: 0.8539 - val_loss: 1.5350 - val_acc: 0.5742\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.57118\n",
            "Epoch 7/25\n",
            "33/33 [==============================] - 25s 758ms/step - loss: 0.2501 - acc: 0.8978 - val_loss: 1.9917 - val_acc: 0.5703\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.57118\n",
            "Epoch 8/25\n",
            "33/33 [==============================] - 25s 756ms/step - loss: 0.1614 - acc: 0.9367 - val_loss: 3.4571 - val_acc: 0.5625\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.57118\n",
            "Epoch 9/25\n",
            "33/33 [==============================] - 25s 757ms/step - loss: 0.1838 - acc: 0.9318 - val_loss: 1.8861 - val_acc: 0.5547\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.57118\n",
            "Epoch 10/25\n",
            "33/33 [==============================] - 25s 758ms/step - loss: 0.1247 - acc: 0.9523 - val_loss: 1.5763 - val_acc: 0.6406\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.57118\n",
            "Epoch 11/25\n",
            "33/33 [==============================] - 25s 754ms/step - loss: 0.1792 - acc: 0.9318 - val_loss: 0.6794 - val_acc: 0.8008\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.57118\n",
            "Epoch 12/25\n",
            "33/33 [==============================] - 25s 748ms/step - loss: 0.1412 - acc: 0.9416 - val_loss: 1.3263 - val_acc: 0.6992\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.57118\n",
            "Epoch 13/25\n",
            "33/33 [==============================] - 25s 751ms/step - loss: 0.0941 - acc: 0.9630 - val_loss: 1.8563 - val_acc: 0.6914\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.57118\n",
            "Epoch 14/25\n",
            "33/33 [==============================] - 25s 749ms/step - loss: 0.0741 - acc: 0.9776 - val_loss: 3.1491 - val_acc: 0.5586\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.57118\n",
            "Epoch 15/25\n",
            "33/33 [==============================] - 25s 749ms/step - loss: 0.0640 - acc: 0.9786 - val_loss: 2.7295 - val_acc: 0.6289\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.57118\n",
            "Epoch 16/25\n",
            "33/33 [==============================] - 25s 749ms/step - loss: 0.2474 - acc: 0.9163 - val_loss: 0.9755 - val_acc: 0.8203\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.57118\n",
            "Epoch 17/25\n",
            "33/33 [==============================] - 25s 750ms/step - loss: 0.1429 - acc: 0.9552 - val_loss: 1.2931 - val_acc: 0.6484\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.57118\n",
            "Epoch 18/25\n",
            "33/33 [==============================] - 25s 749ms/step - loss: 0.1004 - acc: 0.9552 - val_loss: 1.8301 - val_acc: 0.6875\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.57118\n",
            "Epoch 19/25\n",
            "33/33 [==============================] - 25s 752ms/step - loss: 0.0666 - acc: 0.9747 - val_loss: 0.9845 - val_acc: 0.8086\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.57118\n",
            "Epoch 20/25\n",
            "33/33 [==============================] - 25s 748ms/step - loss: 0.0562 - acc: 0.9786 - val_loss: 1.4714 - val_acc: 0.6992\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.57118\n",
            "Epoch 21/25\n",
            "33/33 [==============================] - 25s 748ms/step - loss: 0.0500 - acc: 0.9864 - val_loss: 1.5638 - val_acc: 0.7617\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.57118\n",
            "Epoch 22/25\n",
            "33/33 [==============================] - 25s 749ms/step - loss: 0.0652 - acc: 0.9796 - val_loss: 1.3679 - val_acc: 0.7617\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.57118\n",
            "Epoch 23/25\n",
            "33/33 [==============================] - 25s 748ms/step - loss: 0.0423 - acc: 0.9854 - val_loss: 1.1033 - val_acc: 0.8164\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.57118\n",
            "Epoch 24/25\n",
            "33/33 [==============================] - 25s 749ms/step - loss: 0.0598 - acc: 0.9776 - val_loss: 3.4827 - val_acc: 0.6016\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.57118\n",
            "Epoch 25/25\n",
            "33/33 [==============================] - 25s 750ms/step - loss: 0.0870 - acc: 0.9737 - val_loss: 0.1526 - val_acc: 0.9688\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.57118 to 0.15264, saving model to tmp_checkpoint.ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5quLZQSgB_Cf"
      },
      "source": [
        "#TF3-horsesorhumans-B-2-val-loss-0.2945\n",
        "\n",
        "max pooling(2, 2) 대신 strides = (2, 2)도 시도해보기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIQtTqtMDSMV",
        "outputId": "c6499e03-aafa-42ad-edaf-b35e3c4de15a"
      },
      "source": [
        "# Question\n",
        "#\n",
        "# This task requires you to create a classifier for horses or humans using\n",
        "# the provided data. Please make sure your final layer is a 1 neuron, activated by sigmoid as shown.\n",
        "# Please note that the test will use images that are 300x300 with 3 bytes color depth so be sure to design your neural network accordingly\n",
        "\n",
        "# =========== 합격 기준 가이드라인 공유 ============= #\n",
        "# val_loss 기준에 맞춰 주시는 것이 훨씬 더 중요 #\n",
        "# val_loss 보다 조금 높아도 상관없음. (언저리까지 OK) #\n",
        "# =================================================== #\n",
        "# 문제명: Category 3 - Horses Or Humans (Type B)\n",
        "# val_loss: 0.51 (더 낮아도 안 좋고, 높아도 안 좋음!)\n",
        "# val_acc: 관계없음\n",
        "# =================================================== #\n",
        "# =================================================== #\n",
        "\n",
        "\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import numpy as np\n",
        "from IPython.display import Image\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dropout, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "def solution_model():\n",
        "    _TRAIN_URL = \"https://storage.googleapis.com/download.tensorflow.org/data/horse-or-human.zip\"\n",
        "    _TEST_URL = \"https://storage.googleapis.com/download.tensorflow.org/data/validation-horse-or-human.zip\"\n",
        "    urllib.request.urlretrieve(_TRAIN_URL, 'horse-or-human.zip')\n",
        "    local_zip = 'horse-or-human.zip'\n",
        "    zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "    zip_ref.extractall('tmp/horse-or-human/')\n",
        "    zip_ref.close()\n",
        "    urllib.request.urlretrieve(_TEST_URL, 'validation-horse-or-human.zip')\n",
        "    local_zip = 'validation-horse-or-human.zip'\n",
        "    zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "    zip_ref.extractall('tmp/validation-horse-or-human/')\n",
        "    zip_ref.close()\n",
        "\n",
        "    TRAINING_DIR = 'tmp/horse-or-human/'\n",
        "    VALIDATION_DIR = 'tmp/validation-horse-or-human/'\n",
        "\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        #Your code here. Should at least have a rescale. Other parameters can help with overfitting.\n",
        "            rescale=1/ 255.0,\n",
        "            rotation_range=5,\n",
        "            width_shift_range=0.05,\n",
        "            height_shift_range=0.1,\n",
        "            zoom_range=0.05,\n",
        "            horizontal_flip=True,\n",
        "            fill_mode='nearest',\n",
        "        )\n",
        "\n",
        "    validation_datagen = ImageDataGenerator(\n",
        "        #Your Code here\n",
        "         rescale=1/ 255.0,)\n",
        "\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        #Your Code Here\n",
        "          TRAINING_DIR,\n",
        "          target_size=(300, 300),\n",
        "          batch_size=32,\n",
        "          class_mode='binary',\n",
        "          )\n",
        "\n",
        "    validation_generator = validation_datagen.flow_from_directory(\n",
        "        #Your Code Here\n",
        "          VALIDATION_DIR,\n",
        "          target_size=(300, 300),\n",
        "          batch_size=32,\n",
        "          class_mode='binary',\n",
        "          )\n",
        "\n",
        "\n",
        "    model = tf.keras.models.Sequential([\n",
        "        # Note the input shape specified on your first layer must be (300,300,3)\n",
        "        # Your Code here\n",
        "        Conv2D(16, (3, 3), strides = (2, 2), activation='relu', input_shape=(300, 300, 3)),\n",
        "        Conv2D(32, (3, 3), strides = (2, 2), activation='relu'),\n",
        "        Conv2D(64, (3, 3), strides = (2, 2), activation='relu'),\n",
        "        Conv2D(64, (3, 3), strides = (2, 2), activation='relu'),\n",
        "        Conv2D(128, (3, 3), strides = (2, 2), activation='relu'),\n",
        "        Flatten(),\n",
        "        Dropout(0.5),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dense(32, activation='relu'),\n",
        "        # This is the last layer. You should not change this code.\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "    checkpoint_path = \"tmp_checkpoint.ckpt\"\n",
        "    checkpoint = ModelCheckpoint(filepath=checkpoint_path, \n",
        "                                save_weights_only=True, \n",
        "                                save_best_only=True, \n",
        "                                monitor='val_loss', \n",
        "                                verbose=1)\n",
        "\n",
        "    model.fit(train_generator, \n",
        "              validation_data=(validation_generator),\n",
        "              epochs=25,\n",
        "              callbacks=[checkpoint],\n",
        "              )\n",
        "    model.load_weights(checkpoint_path)\n",
        "    # NOTE: If training is taking a very long time, you should consider setting the batch size appropriately on the generator, and the steps per epoch in the model.fit#\n",
        "    return model\n",
        "\n",
        "\n",
        "# Note that you'll need to save your model as a .h5 like this\n",
        "# This .h5 will be uploaded to the testing infrastructure\n",
        "# and a score will be returned to you\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"mymodel.h5\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1027 images belonging to 2 classes.\n",
            "Found 256 images belonging to 2 classes.\n",
            "Epoch 1/25\n",
            "33/33 [==============================] - 26s 757ms/step - loss: 0.6251 - acc: 0.6241 - val_loss: 0.6395 - val_acc: 0.5273\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.63954, saving model to tmp_checkpoint.ckpt\n",
            "Epoch 2/25\n",
            "33/33 [==============================] - 25s 748ms/step - loss: 0.3361 - acc: 0.8549 - val_loss: 1.1993 - val_acc: 0.5938\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.63954\n",
            "Epoch 3/25\n",
            "33/33 [==============================] - 25s 748ms/step - loss: 0.2448 - acc: 0.9124 - val_loss: 1.5111 - val_acc: 0.6641\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.63954\n",
            "Epoch 4/25\n",
            "33/33 [==============================] - 25s 742ms/step - loss: 0.1806 - acc: 0.9241 - val_loss: 0.3777 - val_acc: 0.8633\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.63954 to 0.37772, saving model to tmp_checkpoint.ckpt\n",
            "Epoch 5/25\n",
            "33/33 [==============================] - 24s 740ms/step - loss: 0.2029 - acc: 0.9270 - val_loss: 1.2680 - val_acc: 0.7617\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.37772\n",
            "Epoch 6/25\n",
            "33/33 [==============================] - 24s 737ms/step - loss: 0.1296 - acc: 0.9581 - val_loss: 1.6589 - val_acc: 0.7109\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.37772\n",
            "Epoch 7/25\n",
            "33/33 [==============================] - 24s 735ms/step - loss: 0.0917 - acc: 0.9620 - val_loss: 1.9646 - val_acc: 0.7266\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.37772\n",
            "Epoch 8/25\n",
            "33/33 [==============================] - 24s 737ms/step - loss: 0.0401 - acc: 0.9873 - val_loss: 2.4916 - val_acc: 0.7617\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.37772\n",
            "Epoch 9/25\n",
            "33/33 [==============================] - 24s 733ms/step - loss: 0.0441 - acc: 0.9834 - val_loss: 3.5223 - val_acc: 0.7031\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.37772\n",
            "Epoch 10/25\n",
            "33/33 [==============================] - 24s 733ms/step - loss: 0.0725 - acc: 0.9718 - val_loss: 1.7699 - val_acc: 0.8516\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.37772\n",
            "Epoch 11/25\n",
            "33/33 [==============================] - 24s 735ms/step - loss: 0.1346 - acc: 0.9552 - val_loss: 1.7824 - val_acc: 0.8047\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.37772\n",
            "Epoch 12/25\n",
            "33/33 [==============================] - 24s 733ms/step - loss: 0.0569 - acc: 0.9805 - val_loss: 1.8548 - val_acc: 0.7969\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.37772\n",
            "Epoch 13/25\n",
            "33/33 [==============================] - 24s 733ms/step - loss: 0.0470 - acc: 0.9864 - val_loss: 3.8304 - val_acc: 0.7031\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.37772\n",
            "Epoch 14/25\n",
            "33/33 [==============================] - 24s 735ms/step - loss: 0.0533 - acc: 0.9796 - val_loss: 4.0588 - val_acc: 0.6758\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.37772\n",
            "Epoch 15/25\n",
            "33/33 [==============================] - 24s 731ms/step - loss: 0.0432 - acc: 0.9883 - val_loss: 2.5117 - val_acc: 0.7266\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.37772\n",
            "Epoch 16/25\n",
            "33/33 [==============================] - 24s 731ms/step - loss: 0.0278 - acc: 0.9912 - val_loss: 2.1324 - val_acc: 0.7617\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.37772\n",
            "Epoch 17/25\n",
            "33/33 [==============================] - 24s 734ms/step - loss: 0.0279 - acc: 0.9854 - val_loss: 2.4690 - val_acc: 0.7852\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.37772\n",
            "Epoch 18/25\n",
            "33/33 [==============================] - 24s 731ms/step - loss: 0.0473 - acc: 0.9883 - val_loss: 2.2323 - val_acc: 0.7734\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.37772\n",
            "Epoch 19/25\n",
            "33/33 [==============================] - 24s 731ms/step - loss: 0.0691 - acc: 0.9834 - val_loss: 2.4060 - val_acc: 0.7266\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.37772\n",
            "Epoch 20/25\n",
            "33/33 [==============================] - 24s 728ms/step - loss: 0.0255 - acc: 0.9942 - val_loss: 3.3582 - val_acc: 0.7539\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.37772\n",
            "Epoch 21/25\n",
            "33/33 [==============================] - 24s 731ms/step - loss: 0.1530 - acc: 0.9474 - val_loss: 0.2945 - val_acc: 0.9141\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.37772 to 0.29455, saving model to tmp_checkpoint.ckpt\n",
            "Epoch 22/25\n",
            "33/33 [==============================] - 24s 731ms/step - loss: 0.1345 - acc: 0.9435 - val_loss: 1.7768 - val_acc: 0.8086\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.29455\n",
            "Epoch 23/25\n",
            "33/33 [==============================] - 24s 729ms/step - loss: 0.0927 - acc: 0.9679 - val_loss: 1.4716 - val_acc: 0.8398\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.29455\n",
            "Epoch 24/25\n",
            "33/33 [==============================] - 24s 727ms/step - loss: 0.1852 - acc: 0.9348 - val_loss: 0.7934 - val_acc: 0.8789\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.29455\n",
            "Epoch 25/25\n",
            "33/33 [==============================] - 24s 728ms/step - loss: 0.0549 - acc: 0.9776 - val_loss: 2.8882 - val_acc: 0.6875\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.29455\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZLJuCg7CF79"
      },
      "source": [
        "#TF3-horsesorhumans-B-3-val-loss-0.246\n",
        "\n",
        "max pooling(2, 2) 대신 strides = (2, 2)도 시도해보기\n",
        "+\n",
        "rmsprop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGcjr9c8Dc_6",
        "outputId": "a077d3eb-3da4-43d6-940e-55bc1ba23402"
      },
      "source": [
        "# Question\n",
        "#\n",
        "# This task requires you to create a classifier for horses or humans using\n",
        "# the provided data. Please make sure your final layer is a 1 neuron, activated by sigmoid as shown.\n",
        "# Please note that the test will use images that are 300x300 with 3 bytes color depth so be sure to design your neural network accordingly\n",
        "\n",
        "# =========== 합격 기준 가이드라인 공유 ============= #\n",
        "# val_loss 기준에 맞춰 주시는 것이 훨씬 더 중요 #\n",
        "# val_loss 보다 조금 높아도 상관없음. (언저리까지 OK) #\n",
        "# =================================================== #\n",
        "# 문제명: Category 3 - Horses Or Humans (Type B)\n",
        "# val_loss: 0.51 (더 낮아도 안 좋고, 높아도 안 좋음!)\n",
        "# val_acc: 관계없음\n",
        "# =================================================== #\n",
        "# =================================================== #\n",
        "\n",
        "\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import numpy as np\n",
        "from IPython.display import Image\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dropout, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "def solution_model():\n",
        "    _TRAIN_URL = \"https://storage.googleapis.com/download.tensorflow.org/data/horse-or-human.zip\"\n",
        "    _TEST_URL = \"https://storage.googleapis.com/download.tensorflow.org/data/validation-horse-or-human.zip\"\n",
        "    urllib.request.urlretrieve(_TRAIN_URL, 'horse-or-human.zip')\n",
        "    local_zip = 'horse-or-human.zip'\n",
        "    zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "    zip_ref.extractall('tmp/horse-or-human/')\n",
        "    zip_ref.close()\n",
        "    urllib.request.urlretrieve(_TEST_URL, 'validation-horse-or-human.zip')\n",
        "    local_zip = 'validation-horse-or-human.zip'\n",
        "    zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "    zip_ref.extractall('tmp/validation-horse-or-human/')\n",
        "    zip_ref.close()\n",
        "\n",
        "    TRAINING_DIR = 'tmp/horse-or-human/'\n",
        "    VALIDATION_DIR = 'tmp/validation-horse-or-human/'\n",
        "\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        #Your code here. Should at least have a rescale. Other parameters can help with overfitting.\n",
        "            rescale=1/ 255.0,\n",
        "            rotation_range=5,\n",
        "            width_shift_range=0.05,\n",
        "            height_shift_range=0.1,\n",
        "            zoom_range=0.05,\n",
        "            horizontal_flip=True,\n",
        "            fill_mode='nearest',\n",
        "        )\n",
        "\n",
        "    validation_datagen = ImageDataGenerator(\n",
        "        #Your Code here\n",
        "         rescale=1/ 255.0,)\n",
        "\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        #Your Code Here\n",
        "          TRAINING_DIR,\n",
        "          target_size=(300, 300),\n",
        "          batch_size=32,\n",
        "          class_mode='binary',\n",
        "          )\n",
        "\n",
        "    validation_generator = validation_datagen.flow_from_directory(\n",
        "        #Your Code Here\n",
        "          VALIDATION_DIR,\n",
        "          target_size=(300, 300),\n",
        "          batch_size=32,\n",
        "          class_mode='binary',\n",
        "          )\n",
        "\n",
        "\n",
        "    model = tf.keras.models.Sequential([\n",
        "        # Note the input shape specified on your first layer must be (300,300,3)\n",
        "        # Your Code here\n",
        "        Conv2D(16, (3, 3), strides = (2, 2), activation='relu', input_shape=(300, 300, 3)),\n",
        "        Conv2D(32, (3, 3), strides = (2, 2), activation='relu'),\n",
        "        Conv2D(64, (3, 3), strides = (2, 2), activation='relu'),\n",
        "        Conv2D(64, (3, 3), strides = (2, 2), activation='relu'),\n",
        "        Conv2D(128, (3, 3), strides = (2, 2), activation='relu'),\n",
        "        Flatten(),\n",
        "        Dropout(0.5),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dense(32, activation='relu'),\n",
        "        # This is the last layer. You should not change this code.\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "    checkpoint_path = \"tmp_checkpoint.ckpt\"\n",
        "    checkpoint = ModelCheckpoint(filepath=checkpoint_path, \n",
        "                                save_weights_only=True, \n",
        "                                save_best_only=True, \n",
        "                                monitor='val_loss', \n",
        "                                verbose=1)\n",
        "\n",
        "    model.fit(train_generator, \n",
        "              validation_data=(validation_generator),\n",
        "              epochs=25,\n",
        "              callbacks=[checkpoint],\n",
        "              )\n",
        "    model.load_weights(checkpoint_path)\n",
        "    # NOTE: If training is taking a very long time, you should consider setting the batch size appropriately on the generator, and the steps per epoch in the model.fit#\n",
        "    return model\n",
        "\n",
        "\n",
        "# Note that you'll need to save your model as a .h5 like this\n",
        "# This .h5 will be uploaded to the testing infrastructure\n",
        "# and a score will be returned to you\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"mymodel.h5\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1027 images belonging to 2 classes.\n",
            "Found 256 images belonging to 2 classes.\n",
            "Epoch 1/25\n",
            "33/33 [==============================] - 26s 746ms/step - loss: 0.7090 - acc: 0.5784 - val_loss: 1.7176 - val_acc: 0.5000\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.71764, saving model to tmp_checkpoint.ckpt\n",
            "Epoch 2/25\n",
            "33/33 [==============================] - 24s 739ms/step - loss: 0.7109 - acc: 0.7342 - val_loss: 0.7935 - val_acc: 0.5234\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.71764 to 0.79355, saving model to tmp_checkpoint.ckpt\n",
            "Epoch 3/25\n",
            "33/33 [==============================] - 24s 735ms/step - loss: 0.4552 - acc: 0.8189 - val_loss: 0.8856 - val_acc: 0.6562\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.79355\n",
            "Epoch 4/25\n",
            "33/33 [==============================] - 24s 734ms/step - loss: 0.3691 - acc: 0.8744 - val_loss: 0.9935 - val_acc: 0.7266\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.79355\n",
            "Epoch 5/25\n",
            "33/33 [==============================] - 24s 734ms/step - loss: 0.2646 - acc: 0.8987 - val_loss: 0.7764 - val_acc: 0.8203\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.79355 to 0.77644, saving model to tmp_checkpoint.ckpt\n",
            "Epoch 6/25\n",
            "33/33 [==============================] - 24s 752ms/step - loss: 0.3711 - acc: 0.8948 - val_loss: 0.2460 - val_acc: 0.8867\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.77644 to 0.24602, saving model to tmp_checkpoint.ckpt\n",
            "Epoch 7/25\n",
            "33/33 [==============================] - 24s 731ms/step - loss: 0.2044 - acc: 0.9338 - val_loss: 0.9028 - val_acc: 0.7812\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.24602\n",
            "Epoch 8/25\n",
            "33/33 [==============================] - 24s 730ms/step - loss: 0.1380 - acc: 0.9445 - val_loss: 1.6325 - val_acc: 0.7461\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.24602\n",
            "Epoch 9/25\n",
            "33/33 [==============================] - 24s 730ms/step - loss: 0.1603 - acc: 0.9426 - val_loss: 1.3872 - val_acc: 0.8047\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.24602\n",
            "Epoch 10/25\n",
            "33/33 [==============================] - 24s 732ms/step - loss: 0.2800 - acc: 0.9533 - val_loss: 2.4512 - val_acc: 0.7461\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.24602\n",
            "Epoch 11/25\n",
            "33/33 [==============================] - 24s 730ms/step - loss: 0.0723 - acc: 0.9786 - val_loss: 3.1293 - val_acc: 0.7578\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.24602\n",
            "Epoch 12/25\n",
            "33/33 [==============================] - 24s 731ms/step - loss: 0.1374 - acc: 0.9591 - val_loss: 1.7807 - val_acc: 0.7969\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.24602\n",
            "Epoch 13/25\n",
            "33/33 [==============================] - 24s 727ms/step - loss: 0.0873 - acc: 0.9747 - val_loss: 1.7003 - val_acc: 0.8008\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.24602\n",
            "Epoch 14/25\n",
            "33/33 [==============================] - 24s 726ms/step - loss: 0.1220 - acc: 0.9659 - val_loss: 0.8864 - val_acc: 0.8750\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.24602\n",
            "Epoch 15/25\n",
            "33/33 [==============================] - 24s 726ms/step - loss: 0.0392 - acc: 0.9893 - val_loss: 3.8432 - val_acc: 0.7539\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.24602\n",
            "Epoch 16/25\n",
            "33/33 [==============================] - 24s 727ms/step - loss: 0.1768 - acc: 0.9601 - val_loss: 2.2013 - val_acc: 0.7266\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.24602\n",
            "Epoch 17/25\n",
            "33/33 [==============================] - 24s 727ms/step - loss: 0.2339 - acc: 0.9562 - val_loss: 3.6628 - val_acc: 0.6953\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.24602\n",
            "Epoch 18/25\n",
            "33/33 [==============================] - 24s 724ms/step - loss: 0.0481 - acc: 0.9825 - val_loss: 3.6107 - val_acc: 0.7500\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.24602\n",
            "Epoch 19/25\n",
            "33/33 [==============================] - 24s 728ms/step - loss: 0.2260 - acc: 0.9757 - val_loss: 3.2486 - val_acc: 0.7227\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.24602\n",
            "Epoch 20/25\n",
            "33/33 [==============================] - 24s 729ms/step - loss: 0.0108 - acc: 0.9951 - val_loss: 6.4827 - val_acc: 0.6953\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.24602\n",
            "Epoch 21/25\n",
            "33/33 [==============================] - 24s 724ms/step - loss: 0.3368 - acc: 0.9533 - val_loss: 2.0969 - val_acc: 0.7617\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.24602\n",
            "Epoch 22/25\n",
            "33/33 [==============================] - 24s 724ms/step - loss: 0.0239 - acc: 0.9932 - val_loss: 0.5287 - val_acc: 0.9219\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.24602\n",
            "Epoch 23/25\n",
            "33/33 [==============================] - 24s 725ms/step - loss: 0.0434 - acc: 0.9815 - val_loss: 2.3294 - val_acc: 0.7930\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.24602\n",
            "Epoch 24/25\n",
            "33/33 [==============================] - 24s 726ms/step - loss: 0.0198 - acc: 0.9922 - val_loss: 5.9338 - val_acc: 0.6719\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.24602\n",
            "Epoch 25/25\n",
            "33/33 [==============================] - 24s 723ms/step - loss: 0.3205 - acc: 0.9786 - val_loss: 2.0357 - val_acc: 0.8516\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.24602\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAOjIL9YCGCA"
      },
      "source": [
        "#TF3-horsesorhumans-B-4-val-loss-0.3163\n",
        "\n",
        "기본에 rmsprop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGLgAXxCU5X_",
        "outputId": "ec8ae1ec-8118-475b-cc85-dd03421b902e"
      },
      "source": [
        "# Question\n",
        "#\n",
        "# This task requires you to create a classifier for horses or humans using\n",
        "# the provided data. Please make sure your final layer is a 1 neuron, activated by sigmoid as shown.\n",
        "# Please note that the test will use images that are 300x300 with 3 bytes color depth so be sure to design your neural network accordingly\n",
        "\n",
        "# =========== 합격 기준 가이드라인 공유 ============= #\n",
        "# val_loss 기준에 맞춰 주시는 것이 훨씬 더 중요 #\n",
        "# val_loss 보다 조금 높아도 상관없음. (언저리까지 OK) #\n",
        "# =================================================== #\n",
        "# 문제명: Category 3 - Horses Or Humans (Type B)\n",
        "# val_loss: 0.51 (더 낮아도 안 좋고, 높아도 안 좋음!)\n",
        "# val_acc: 관계없음\n",
        "# =================================================== #\n",
        "# =================================================== #\n",
        "\n",
        "\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import numpy as np\n",
        "from IPython.display import Image\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dropout, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "def solution_model():\n",
        "    _TRAIN_URL = \"https://storage.googleapis.com/download.tensorflow.org/data/horse-or-human.zip\"\n",
        "    _TEST_URL = \"https://storage.googleapis.com/download.tensorflow.org/data/validation-horse-or-human.zip\"\n",
        "    urllib.request.urlretrieve(_TRAIN_URL, 'horse-or-human.zip')\n",
        "    local_zip = 'horse-or-human.zip'\n",
        "    zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "    zip_ref.extractall('tmp/horse-or-human/')\n",
        "    zip_ref.close()\n",
        "    urllib.request.urlretrieve(_TEST_URL, 'validation-horse-or-human.zip')\n",
        "    local_zip = 'validation-horse-or-human.zip'\n",
        "    zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "    zip_ref.extractall('tmp/validation-horse-or-human/')\n",
        "    zip_ref.close()\n",
        "\n",
        "    TRAINING_DIR = 'tmp/horse-or-human/'\n",
        "    VALIDATION_DIR = 'tmp/validation-horse-or-human/'\n",
        "\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        #Your code here. Should at least have a rescale. Other parameters can help with overfitting.\n",
        "            rescale=1/ 255.0,\n",
        "            rotation_range=30,\n",
        "            width_shift_range=0.3,\n",
        "            height_shift_range=0.1,\n",
        "            shear_range=0.1,\n",
        "            zoom_range=0.3,\n",
        "            horizontal_flip=True,\n",
        "            fill_mode='nearest',\n",
        "        )\n",
        "\n",
        "    validation_datagen = ImageDataGenerator(\n",
        "        #Your Code here\n",
        "         rescale=1/ 255.0,)\n",
        "\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        #Your Code Here\n",
        "          TRAINING_DIR,\n",
        "          target_size=(300, 300),\n",
        "          batch_size=32,\n",
        "          class_mode='binary',\n",
        "          )\n",
        "\n",
        "    validation_generator = validation_datagen.flow_from_directory(\n",
        "        #Your Code Here\n",
        "          VALIDATION_DIR,\n",
        "          target_size=(300, 300),\n",
        "          batch_size=32,\n",
        "          class_mode='binary',\n",
        "          )\n",
        "\n",
        "\n",
        "    model = tf.keras.models.Sequential([\n",
        "        # Note the input shape specified on your first layer must be (300,300,3)\n",
        "        # Your Code here\n",
        "        Conv2D(16, (3, 3), activation='relu', input_shape=(300, 300, 3)),\n",
        "        MaxPooling2D(2, 2),\n",
        "        Conv2D(32, (3, 3), activation='relu'),\n",
        "        MaxPooling2D(2, 2),\n",
        "        Conv2D(64, (3, 3), activation='relu'),\n",
        "        MaxPooling2D(2, 2),\n",
        "        Conv2D(64, (3, 3), activation='relu'),\n",
        "        MaxPooling2D(2, 2),\n",
        "        Conv2D(128, (3, 3), activation='relu'),\n",
        "        MaxPooling2D(2, 2),\n",
        "        Flatten(),\n",
        "        Dropout(0.5),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dense(32, activation='relu'),\n",
        "        # This is the last layer. You should not change this code.\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "    checkpoint_path = \"tmp_checkpoint.ckpt\"\n",
        "    checkpoint = ModelCheckpoint(filepath=checkpoint_path, \n",
        "                                save_weights_only=True, \n",
        "                                save_best_only=True, \n",
        "                                monitor='val_loss', \n",
        "                                verbose=1)\n",
        "\n",
        "    model.fit(train_generator, \n",
        "              validation_data=(validation_generator),\n",
        "              epochs=25,\n",
        "              callbacks=[checkpoint],\n",
        "              )\n",
        "    model.load_weights(checkpoint_path)\n",
        "    # NOTE: If training is taking a very long time, you should consider setting the batch size appropriately on the generator, and the steps per epoch in the model.fit#\n",
        "    return model\n",
        "\n",
        "\n",
        "# Note that you'll need to save your model as a .h5 like this\n",
        "# This .h5 will be uploaded to the testing infrastructure\n",
        "# and a score will be returned to you\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"mymodel.h5\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1027 images belonging to 2 classes.\n",
            "Found 256 images belonging to 2 classes.\n",
            "Epoch 1/25\n",
            "33/33 [==============================] - 26s 757ms/step - loss: 0.6979 - acc: 0.5268 - val_loss: 1.2439 - val_acc: 0.5000\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.24394, saving model to tmp_checkpoint.ckpt\n",
            "Epoch 2/25\n",
            "33/33 [==============================] - 25s 747ms/step - loss: 0.6568 - acc: 0.6280 - val_loss: 1.0687 - val_acc: 0.5000\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.24394 to 1.06874, saving model to tmp_checkpoint.ckpt\n",
            "Epoch 3/25\n",
            "33/33 [==============================] - 25s 746ms/step - loss: 0.5695 - acc: 0.7186 - val_loss: 0.7578 - val_acc: 0.5000\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.06874 to 0.75784, saving model to tmp_checkpoint.ckpt\n",
            "Epoch 4/25\n",
            "33/33 [==============================] - 25s 746ms/step - loss: 0.5368 - acc: 0.7361 - val_loss: 0.4928 - val_acc: 0.7695\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.75784 to 0.49277, saving model to tmp_checkpoint.ckpt\n",
            "Epoch 5/25\n",
            "33/33 [==============================] - 25s 744ms/step - loss: 0.4697 - acc: 0.8043 - val_loss: 1.4416 - val_acc: 0.5781\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.49277\n",
            "Epoch 6/25\n",
            "33/33 [==============================] - 25s 745ms/step - loss: 0.3709 - acc: 0.8500 - val_loss: 4.0679 - val_acc: 0.5352\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.49277\n",
            "Epoch 7/25\n",
            "33/33 [==============================] - 25s 746ms/step - loss: 0.3087 - acc: 0.8685 - val_loss: 1.1066 - val_acc: 0.6602\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.49277\n",
            "Epoch 8/25\n",
            "33/33 [==============================] - 25s 742ms/step - loss: 0.2709 - acc: 0.8744 - val_loss: 0.7060 - val_acc: 0.8047\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.49277\n",
            "Epoch 9/25\n",
            "33/33 [==============================] - 25s 744ms/step - loss: 0.2775 - acc: 0.9017 - val_loss: 0.8041 - val_acc: 0.8086\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.49277\n",
            "Epoch 10/25\n",
            "33/33 [==============================] - 25s 743ms/step - loss: 0.1801 - acc: 0.9309 - val_loss: 1.1711 - val_acc: 0.8203\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.49277\n",
            "Epoch 11/25\n",
            "33/33 [==============================] - 25s 748ms/step - loss: 0.1910 - acc: 0.9279 - val_loss: 0.9459 - val_acc: 0.7930\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.49277\n",
            "Epoch 12/25\n",
            "33/33 [==============================] - 25s 745ms/step - loss: 0.1738 - acc: 0.9328 - val_loss: 0.3163 - val_acc: 0.9180\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.49277 to 0.31632, saving model to tmp_checkpoint.ckpt\n",
            "Epoch 13/25\n",
            "33/33 [==============================] - 25s 747ms/step - loss: 0.1392 - acc: 0.9513 - val_loss: 1.1333 - val_acc: 0.7969\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.31632\n",
            "Epoch 14/25\n",
            "33/33 [==============================] - 25s 745ms/step - loss: 0.1596 - acc: 0.9387 - val_loss: 0.5160 - val_acc: 0.8945\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.31632\n",
            "Epoch 15/25\n",
            "33/33 [==============================] - 25s 743ms/step - loss: 0.1459 - acc: 0.9494 - val_loss: 1.3590 - val_acc: 0.7578\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.31632\n",
            "Epoch 16/25\n",
            "33/33 [==============================] - 25s 747ms/step - loss: 0.0806 - acc: 0.9747 - val_loss: 1.0098 - val_acc: 0.8242\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.31632\n",
            "Epoch 17/25\n",
            "33/33 [==============================] - 25s 742ms/step - loss: 0.1422 - acc: 0.9630 - val_loss: 0.4384 - val_acc: 0.8867\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.31632\n",
            "Epoch 18/25\n",
            "33/33 [==============================] - 24s 740ms/step - loss: 0.0973 - acc: 0.9698 - val_loss: 0.7330 - val_acc: 0.8828\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.31632\n",
            "Epoch 19/25\n",
            "33/33 [==============================] - 25s 742ms/step - loss: 0.1236 - acc: 0.9669 - val_loss: 1.4406 - val_acc: 0.8398\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.31632\n",
            "Epoch 20/25\n",
            "33/33 [==============================] - 25s 744ms/step - loss: 0.1076 - acc: 0.9669 - val_loss: 0.9701 - val_acc: 0.8711\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.31632\n",
            "Epoch 21/25\n",
            "33/33 [==============================] - 25s 745ms/step - loss: 0.0933 - acc: 0.9786 - val_loss: 0.6308 - val_acc: 0.8906\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.31632\n",
            "Epoch 22/25\n",
            "33/33 [==============================] - 24s 738ms/step - loss: 0.1346 - acc: 0.9640 - val_loss: 1.5822 - val_acc: 0.7891\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.31632\n",
            "Epoch 23/25\n",
            "33/33 [==============================] - 25s 741ms/step - loss: 0.0440 - acc: 0.9854 - val_loss: 1.0337 - val_acc: 0.8555\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.31632\n",
            "Epoch 24/25\n",
            "33/33 [==============================] - 25s 745ms/step - loss: 0.1233 - acc: 0.9698 - val_loss: 1.6164 - val_acc: 0.7461\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.31632\n",
            "Epoch 25/25\n",
            "33/33 [==============================] - 25s 746ms/step - loss: 0.1048 - acc: 0.9718 - val_loss: 1.0090 - val_acc: 0.8516\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.31632\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}