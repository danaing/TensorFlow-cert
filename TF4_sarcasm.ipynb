{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF4-sarcasm",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danaing/TensorFlow-cert/blob/main/TF4_sarcasm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILex-rY6vROD"
      },
      "source": [
        "# TF4-sarcasm-1-val-loss-0.3743"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nrt5mX8ZbiGu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fefd1d2-7767-42ec-81ff-b21b9bc6cfa2"
      },
      "source": [
        "# ======================================================================\n",
        "# There are 5 questions in this test with increasing difficulty from 1-5\n",
        "# Please note that the weight of the grade for the question is relative\n",
        "# to its difficulty. So your Category 1 question will score much less\n",
        "# than your Category 5 question.\n",
        "# ======================================================================\n",
        "#\n",
        "# NLP QUESTION\n",
        "#\n",
        "# For this task you will build a classifier for the sarcasm dataset\n",
        "# The classifier should have a final layer with 1 neuron activated by sigmoid as shown\n",
        "# It will be tested against a number of sentences that the network hasn't previously seen\n",
        "# And you will be scored on whether sarcasm was correctly detected in those sentences\n",
        "\n",
        "# =========== 합격 기준 가이드라인 공유 ============= #\n",
        "# val_loss 기준에 맞춰 주시는 것이 훨씬 더 중요 #\n",
        "# val_loss 보다 조금 높아도 상관없음. (언저리까지 OK) #\n",
        "# =================================================== #\n",
        "# 문제명: Category 4 - sarcasm\n",
        "# val_loss: 0.3650\n",
        "# val_acc: 0.83\n",
        "# =================================================== #\n",
        "# =================================================== #\n",
        "\n",
        "\n",
        "import json\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import urllib\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Flatten\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "\n",
        "def solution_model():\n",
        "    url = 'https://storage.googleapis.com/download.tensorflow.org/data/sarcasm.json'\n",
        "    urllib.request.urlretrieve(url, 'sarcasm.json')\n",
        "    with open('sarcasm.json') as f:\n",
        "        datas = json.load(f)\n",
        "        \n",
        "    # DO NOT CHANGE THIS CODE OR THE TESTS MAY NOT WORK\n",
        "    vocab_size = 1000\n",
        "    embedding_dim = 16\n",
        "    max_length = 120\n",
        "    trunc_type='post'\n",
        "    padding_type='post'\n",
        "    oov_tok = \"<OOV>\"\n",
        "    training_size = 20000\n",
        "\n",
        "    sentences = []\n",
        "    labels = []\n",
        "\n",
        "      # YOUR CODE HERE\n",
        "    for data in datas:\n",
        "        sentences.append(data['headline'])\n",
        "        labels.append(data['is_sarcastic'])\n",
        "\n",
        "    training_size = 20000\n",
        "    train_sentences = sentences[:training_size]\n",
        "    train_labels = labels[:training_size]\n",
        "    validation_sentences = sentences[training_size:]\n",
        "    validation_labels = labels[training_size:]\n",
        "\n",
        "    tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')\n",
        "\n",
        "    tokenizer.fit_on_texts(train_sentences)\n",
        "\n",
        "    train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
        "    validation_sequences = tokenizer.texts_to_sequences(validation_sentences)\n",
        "\n",
        "    train_padded = pad_sequences(train_sequences, maxlen=max_length, truncating=trunc_type, padding=padding_type)\n",
        "    validation_padded = pad_sequences(validation_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "    train_labels = np.array(train_labels)\n",
        "    validation_labels = np.array(validation_labels)\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "              # YOUR CODE HERE. KEEP THIS OUTPUT LAYER INTACT OR TESTS MAY FAIL\n",
        "          Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "          Bidirectional(LSTM(64, return_sequences=True)),\n",
        "          Bidirectional(LSTM(64)),\n",
        "          Dense(32, activation='relu'),\n",
        "          Dense(16, activation='relu'),\n",
        "          Dense(1, activation='sigmoid')\n",
        "      ])\n",
        "\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "    checkpoint_path = 'my_checkpoint.ckpt'\n",
        "    checkpoint = ModelCheckpoint(checkpoint_path, \n",
        "                                save_weights_only=True, \n",
        "                                save_best_only=True, \n",
        "                                monitor='val_loss',\n",
        "                                verbose=1)\n",
        "    \n",
        "    epochs=10\n",
        "    history = model.fit(train_padded, train_labels, \n",
        "                        validation_data=(validation_padded, validation_labels),\n",
        "                        callbacks=[checkpoint],\n",
        "                        epochs=epochs)\n",
        "    \n",
        "    model.load_weights(checkpoint_path)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# Note that you'll need to save your model as a .h5 like this\n",
        "# This .h5 will be uploaded to the testing infrastructure\n",
        "# and a score will be returned to you\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"TF4-sarcasm.h5\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 26s 28ms/step - loss: 0.4463 - acc: 0.7742 - val_loss: 0.3910 - val_acc: 0.8143\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.39095, saving model to my_checkpoint.ckpt\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 16s 26ms/step - loss: 0.3526 - acc: 0.8382 - val_loss: 0.3839 - val_acc: 0.8183\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.39095 to 0.38391, saving model to my_checkpoint.ckpt\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 16s 26ms/step - loss: 0.3296 - acc: 0.8496 - val_loss: 0.3748 - val_acc: 0.8267\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.38391 to 0.37475, saving model to my_checkpoint.ckpt\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 16s 26ms/step - loss: 0.3139 - acc: 0.8585 - val_loss: 0.3870 - val_acc: 0.8287\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.37475\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 16s 26ms/step - loss: 0.3016 - acc: 0.8676 - val_loss: 0.3743 - val_acc: 0.8313\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.37475 to 0.37430, saving model to my_checkpoint.ckpt\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 16s 26ms/step - loss: 0.2894 - acc: 0.8744 - val_loss: 0.3863 - val_acc: 0.8284\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.37430\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 16s 26ms/step - loss: 0.2783 - acc: 0.8806 - val_loss: 0.3767 - val_acc: 0.8283\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.37430\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 16s 26ms/step - loss: 0.2704 - acc: 0.8844 - val_loss: 0.3885 - val_acc: 0.8258\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.37430\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 16s 26ms/step - loss: 0.2591 - acc: 0.8906 - val_loss: 0.3973 - val_acc: 0.8232\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.37430\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 16s 26ms/step - loss: 0.2533 - acc: 0.8924 - val_loss: 0.4407 - val_acc: 0.8171\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.37430\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t608C2Me2-fy"
      },
      "source": [
        "# TF4-sarcasm-2-val-loss-0.374\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlDghKf32-fz",
        "outputId": "43dd0917-fe21-4334-ffda-3acc6273bf57"
      },
      "source": [
        "# ======================================================================\n",
        "# There are 5 questions in this test with increasing difficulty from 1-5\n",
        "# Please note that the weight of the grade for the question is relative\n",
        "# to its difficulty. So your Category 1 question will score much less\n",
        "# than your Category 5 question.\n",
        "# ======================================================================\n",
        "#\n",
        "# NLP QUESTION\n",
        "#\n",
        "# For this task you will build a classifier for the sarcasm dataset\n",
        "# The classifier should have a final layer with 1 neuron activated by sigmoid as shown\n",
        "# It will be tested against a number of sentences that the network hasn't previously seen\n",
        "# And you will be scored on whether sarcasm was correctly detected in those sentences\n",
        "\n",
        "# =========== 합격 기준 가이드라인 공유 ============= #\n",
        "# val_loss 기준에 맞춰 주시는 것이 훨씬 더 중요 #\n",
        "# val_loss 보다 조금 높아도 상관없음. (언저리까지 OK) #\n",
        "# =================================================== #\n",
        "# 문제명: Category 4 - sarcasm\n",
        "# val_loss: 0.3650\n",
        "# val_acc: 0.83\n",
        "# =================================================== #\n",
        "# =================================================== #\n",
        "\n",
        "\n",
        "import json\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import urllib\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Flatten\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "\n",
        "def solution_model():\n",
        "    url = 'https://storage.googleapis.com/download.tensorflow.org/data/sarcasm.json'\n",
        "    urllib.request.urlretrieve(url, 'sarcasm.json')\n",
        "    with open('sarcasm.json') as f:\n",
        "        datas = json.load(f)\n",
        "        \n",
        "    # DO NOT CHANGE THIS CODE OR THE TESTS MAY NOT WORK\n",
        "    vocab_size = 1000\n",
        "    embedding_dim = 16\n",
        "    max_length = 120\n",
        "    trunc_type='post'\n",
        "    padding_type='post'\n",
        "    oov_tok = \"<OOV>\"\n",
        "    training_size = 20000\n",
        "\n",
        "    sentences = []\n",
        "    labels = []\n",
        "\n",
        "      # YOUR CODE HERE\n",
        "    for data in datas:\n",
        "        sentences.append(data['headline'])\n",
        "        labels.append(data['is_sarcastic'])\n",
        "\n",
        "    training_size = 20000\n",
        "    train_sentences = sentences[:training_size]\n",
        "    train_labels = labels[:training_size]\n",
        "    validation_sentences = sentences[training_size:]\n",
        "    validation_labels = labels[training_size:]\n",
        "\n",
        "    tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')\n",
        "\n",
        "    tokenizer.fit_on_texts(train_sentences)\n",
        "\n",
        "    train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
        "    validation_sequences = tokenizer.texts_to_sequences(validation_sentences)\n",
        "\n",
        "    train_padded = pad_sequences(train_sequences, maxlen=max_length, truncating=trunc_type, padding=padding_type)\n",
        "    validation_padded = pad_sequences(validation_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "    train_labels = np.array(train_labels)\n",
        "    validation_labels = np.array(validation_labels)\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "              # YOUR CODE HERE. KEEP THIS OUTPUT LAYER INTACT OR TESTS MAY FAIL\n",
        "          Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "          Bidirectional(LSTM(64, return_sequences=True)),\n",
        "          Bidirectional(LSTM(64)),\n",
        "          Dense(32, activation='relu'),\n",
        "          Dense(16, activation='relu'),\n",
        "          Dense(1, activation='sigmoid')\n",
        "      ])\n",
        "\n",
        "    model.compile(optimizer='RMSprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "    checkpoint_path = 'my_checkpoint.ckpt'\n",
        "    checkpoint = ModelCheckpoint(checkpoint_path, \n",
        "                                save_weights_only=True, \n",
        "                                save_best_only=True, \n",
        "                                monitor='val_loss',\n",
        "                                verbose=1)\n",
        "    \n",
        "    epochs=10\n",
        "    history = model.fit(train_padded, train_labels, \n",
        "                        validation_data=(validation_padded, validation_labels),\n",
        "                        callbacks=[checkpoint],\n",
        "                        epochs=epochs)\n",
        "    \n",
        "    model.load_weights(checkpoint_path)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# Note that you'll need to save your model as a .h5 like this\n",
        "# This .h5 will be uploaded to the testing infrastructure\n",
        "# and a score will be returned to you\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"TF4-sarcasm-2.h5\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 23s 29ms/step - loss: 0.4763 - acc: 0.7586 - val_loss: 0.4209 - val_acc: 0.8168\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.42092, saving model to my_checkpoint.ckpt\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 17s 27ms/step - loss: 0.3986 - acc: 0.8191 - val_loss: 0.4010 - val_acc: 0.8082\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.42092 to 0.40104, saving model to my_checkpoint.ckpt\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 17s 26ms/step - loss: 0.3827 - acc: 0.8262 - val_loss: 0.4009 - val_acc: 0.8162\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.40104 to 0.40090, saving model to my_checkpoint.ckpt\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 16s 26ms/step - loss: 0.3668 - acc: 0.8309 - val_loss: 0.3907 - val_acc: 0.8202\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.40090 to 0.39069, saving model to my_checkpoint.ckpt\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 17s 26ms/step - loss: 0.3617 - acc: 0.8360 - val_loss: 0.3895 - val_acc: 0.8222\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.39069 to 0.38947, saving model to my_checkpoint.ckpt\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 16s 26ms/step - loss: 0.3537 - acc: 0.8376 - val_loss: 0.4035 - val_acc: 0.8243\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.38947\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 16s 26ms/step - loss: 0.3474 - acc: 0.8432 - val_loss: 0.3797 - val_acc: 0.8304\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.38947 to 0.37967, saving model to my_checkpoint.ckpt\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 16s 26ms/step - loss: 0.3349 - acc: 0.8487 - val_loss: 0.3861 - val_acc: 0.8278\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.37967\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 17s 26ms/step - loss: 0.3250 - acc: 0.8524 - val_loss: 0.3805 - val_acc: 0.8305\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.37967\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 17s 26ms/step - loss: 0.3182 - acc: 0.8590 - val_loss: 0.3740 - val_acc: 0.8344\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.37967 to 0.37402, saving model to my_checkpoint.ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQ6b4vnS29Lh"
      },
      "source": [
        "# TF4-sarcasm-3-val-loss-0.3602"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgIu5ghT29Lu",
        "outputId": "03750542-b063-4fd9-be1e-ecd735a3e857"
      },
      "source": [
        "# ======================================================================\n",
        "# There are 5 questions in this test with increasing difficulty from 1-5\n",
        "# Please note that the weight of the grade for the question is relative\n",
        "# to its difficulty. So your Category 1 question will score much less\n",
        "# than your Category 5 question.\n",
        "# ======================================================================\n",
        "#\n",
        "# NLP QUESTION\n",
        "#\n",
        "# For this task you will build a classifier for the sarcasm dataset\n",
        "# The classifier should have a final layer with 1 neuron activated by sigmoid as shown\n",
        "# It will be tested against a number of sentences that the network hasn't previously seen\n",
        "# And you will be scored on whether sarcasm was correctly detected in those sentences\n",
        "\n",
        "# =========== 합격 기준 가이드라인 공유 ============= #\n",
        "# val_loss 기준에 맞춰 주시는 것이 훨씬 더 중요 #\n",
        "# val_loss 보다 조금 높아도 상관없음. (언저리까지 OK) #\n",
        "# =================================================== #\n",
        "# 문제명: Category 4 - sarcasm\n",
        "# val_loss: 0.3650\n",
        "# val_acc: 0.83\n",
        "# =================================================== #\n",
        "# =================================================== #\n",
        "\n",
        "\n",
        "import json\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import urllib\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Flatten\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "\n",
        "def solution_model():\n",
        "    url = 'https://storage.googleapis.com/download.tensorflow.org/data/sarcasm.json'\n",
        "    urllib.request.urlretrieve(url, 'sarcasm.json')\n",
        "    with open('sarcasm.json') as f:\n",
        "        datas = json.load(f)\n",
        "        \n",
        "    # DO NOT CHANGE THIS CODE OR THE TESTS MAY NOT WORK\n",
        "    vocab_size = 1000\n",
        "    embedding_dim = 16\n",
        "    max_length = 120\n",
        "    trunc_type='post'\n",
        "    padding_type='post'\n",
        "    oov_tok = \"<OOV>\"\n",
        "    training_size = 20000\n",
        "\n",
        "    sentences = []\n",
        "    labels = []\n",
        "\n",
        "      # YOUR CODE HERE\n",
        "    for data in datas:\n",
        "        sentences.append(data['headline'])\n",
        "        labels.append(data['is_sarcastic'])\n",
        "\n",
        "    training_size = 20000\n",
        "    train_sentences = sentences[:training_size]\n",
        "    train_labels = labels[:training_size]\n",
        "    validation_sentences = sentences[training_size:]\n",
        "    validation_labels = labels[training_size:]\n",
        "\n",
        "    tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')\n",
        "\n",
        "    tokenizer.fit_on_texts(train_sentences)\n",
        "\n",
        "    train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
        "    validation_sequences = tokenizer.texts_to_sequences(validation_sentences)\n",
        "\n",
        "    train_padded = pad_sequences(train_sequences, maxlen=max_length, truncating=trunc_type, padding=padding_type)\n",
        "    validation_padded = pad_sequences(validation_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "    train_labels = np.array(train_labels)\n",
        "    validation_labels = np.array(validation_labels)\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "              # YOUR CODE HERE. KEEP THIS OUTPUT LAYER INTACT OR TESTS MAY FAIL\n",
        "          Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "          Bidirectional(LSTM(64, return_sequences=True)),\n",
        "          Bidirectional(LSTM(64, return_sequences=True)),\n",
        "          Bidirectional(LSTM(64)),\n",
        "          Dense(32, activation='relu'),\n",
        "          Dense(16, activation='relu'),\n",
        "          Dense(1, activation='sigmoid')\n",
        "      ])\n",
        "\n",
        "    model.compile(optimizer='RMSprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "    checkpoint_path = 'my_checkpoint.ckpt'\n",
        "    checkpoint = ModelCheckpoint(checkpoint_path, \n",
        "                                save_weights_only=True, \n",
        "                                save_best_only=True, \n",
        "                                monitor='val_loss',\n",
        "                                verbose=1)\n",
        "    \n",
        "    epochs=10\n",
        "    history = model.fit(train_padded, train_labels, \n",
        "                        validation_data=(validation_padded, validation_labels),\n",
        "                        callbacks=[checkpoint],\n",
        "                        epochs=epochs)\n",
        "    \n",
        "    model.load_weights(checkpoint_path)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# Note that you'll need to save your model as a .h5 like this\n",
        "# This .h5 will be uploaded to the testing infrastructure\n",
        "# and a score will be returned to you\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"TF4-sarcasm-3.h5\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 35s 42ms/step - loss: 0.4611 - acc: 0.7660 - val_loss: 0.4062 - val_acc: 0.7983\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.40622, saving model to my_checkpoint.ckpt\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 23s 37ms/step - loss: 0.3703 - acc: 0.8318 - val_loss: 0.3962 - val_acc: 0.8150\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.40622 to 0.39615, saving model to my_checkpoint.ckpt\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 24s 39ms/step - loss: 0.4037 - acc: 0.8201 - val_loss: 0.4008 - val_acc: 0.8173\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.39615\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 23s 37ms/step - loss: 0.3472 - acc: 0.8436 - val_loss: 0.4086 - val_acc: 0.8091\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.39615\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 23s 37ms/step - loss: 0.3289 - acc: 0.8526 - val_loss: 0.3646 - val_acc: 0.8326\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.39615 to 0.36460, saving model to my_checkpoint.ckpt\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 23s 37ms/step - loss: 0.3171 - acc: 0.8594 - val_loss: 0.3796 - val_acc: 0.8298\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.36460\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 23s 37ms/step - loss: 0.3078 - acc: 0.8647 - val_loss: 0.3658 - val_acc: 0.8399\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.36460\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 23s 37ms/step - loss: 0.3015 - acc: 0.8684 - val_loss: 0.3632 - val_acc: 0.8366\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.36460 to 0.36318, saving model to my_checkpoint.ckpt\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 23s 37ms/step - loss: 0.2968 - acc: 0.8723 - val_loss: 0.3602 - val_acc: 0.8359\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.36318 to 0.36019, saving model to my_checkpoint.ckpt\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 23s 37ms/step - loss: 0.2899 - acc: 0.8762 - val_loss: 0.3781 - val_acc: 0.8365\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.36019\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xLFYbkL8IWL"
      },
      "source": [
        "# TF4-sarcasm-4-val-loss-0.3731"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctZtay8G8IWP",
        "outputId": "66e16eb9-f8fd-48f9-8fa9-82b970a0c3dd"
      },
      "source": [
        "# ======================================================================\n",
        "# There are 5 questions in this test with increasing difficulty from 1-5\n",
        "# Please note that the weight of the grade for the question is relative\n",
        "# to its difficulty. So your Category 1 question will score much less\n",
        "# than your Category 5 question.\n",
        "# ======================================================================\n",
        "#\n",
        "# NLP QUESTION\n",
        "#\n",
        "# For this task you will build a classifier for the sarcasm dataset\n",
        "# The classifier should have a final layer with 1 neuron activated by sigmoid as shown\n",
        "# It will be tested against a number of sentences that the network hasn't previously seen\n",
        "# And you will be scored on whether sarcasm was correctly detected in those sentences\n",
        "\n",
        "# =========== 합격 기준 가이드라인 공유 ============= #\n",
        "# val_loss 기준에 맞춰 주시는 것이 훨씬 더 중요 #\n",
        "# val_loss 보다 조금 높아도 상관없음. (언저리까지 OK) #\n",
        "# =================================================== #\n",
        "# 문제명: Category 4 - sarcasm\n",
        "# val_loss: 0.3650\n",
        "# val_acc: 0.83\n",
        "# =================================================== #\n",
        "# =================================================== #\n",
        "\n",
        "\n",
        "import json\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import urllib\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Flatten\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "\n",
        "def solution_model():\n",
        "    url = 'https://storage.googleapis.com/download.tensorflow.org/data/sarcasm.json'\n",
        "    urllib.request.urlretrieve(url, 'sarcasm.json')\n",
        "    with open('sarcasm.json') as f:\n",
        "        datas = json.load(f)\n",
        "        \n",
        "    # DO NOT CHANGE THIS CODE OR THE TESTS MAY NOT WORK\n",
        "    vocab_size = 1000\n",
        "    embedding_dim = 16\n",
        "    max_length = 120\n",
        "    trunc_type='post'\n",
        "    padding_type='post'\n",
        "    oov_tok = \"<OOV>\"\n",
        "    training_size = 20000\n",
        "\n",
        "    sentences = []\n",
        "    labels = []\n",
        "\n",
        "      # YOUR CODE HERE\n",
        "    for data in datas:\n",
        "        sentences.append(data['headline'])\n",
        "        labels.append(data['is_sarcastic'])\n",
        "\n",
        "    training_size = 20000\n",
        "    train_sentences = sentences[:training_size]\n",
        "    train_labels = labels[:training_size]\n",
        "    validation_sentences = sentences[training_size:]\n",
        "    validation_labels = labels[training_size:]\n",
        "\n",
        "    tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')\n",
        "\n",
        "    tokenizer.fit_on_texts(train_sentences)\n",
        "\n",
        "    train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
        "    validation_sequences = tokenizer.texts_to_sequences(validation_sentences)\n",
        "\n",
        "    train_padded = pad_sequences(train_sequences, maxlen=max_length, truncating=trunc_type, padding=padding_type)\n",
        "    validation_padded = pad_sequences(validation_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "    train_labels = np.array(train_labels)\n",
        "    validation_labels = np.array(validation_labels)\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "              # YOUR CODE HERE. KEEP THIS OUTPUT LAYER INTACT OR TESTS MAY FAIL\n",
        "          Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "          Bidirectional(LSTM(64, return_sequences=True)),\n",
        "          Bidirectional(LSTM(64, return_sequences=True)),\n",
        "          Dense(32, activation='relu'),\n",
        "          Dense(32, activation='relu'),\n",
        "          Dense(16, activation='relu'),\n",
        "          Dense(4, activation='relu'),\n",
        "          Dense(1, activation='sigmoid')\n",
        "      ])\n",
        "\n",
        "    model.compile(optimizer='RMSprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "    checkpoint_path = 'my_checkpoint.ckpt'\n",
        "    checkpoint = ModelCheckpoint(checkpoint_path, \n",
        "                                save_weights_only=True, \n",
        "                                save_best_only=True, \n",
        "                                monitor='val_loss',\n",
        "                                verbose=1)\n",
        "    \n",
        "    epochs=10\n",
        "    history = model.fit(train_padded, train_labels, \n",
        "                        validation_data=(validation_padded, validation_labels),\n",
        "                        callbacks=[checkpoint],\n",
        "                        epochs=epochs)\n",
        "    \n",
        "    model.load_weights(checkpoint_path)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# Note that you'll need to save your model as a .h5 like this\n",
        "# This .h5 will be uploaded to the testing infrastructure\n",
        "# and a score will be returned to you\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    model.save(\"TF4-sarcasm-4.h5\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 25s 31ms/step - loss: 0.5463 - acc: 0.7227 - val_loss: 0.4533 - val_acc: 0.7819\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.45332, saving model to my_checkpoint.ckpt\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 17s 28ms/step - loss: 0.4315 - acc: 0.8144 - val_loss: 0.4182 - val_acc: 0.8184\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.45332 to 0.41824, saving model to my_checkpoint.ckpt\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 17s 28ms/step - loss: 0.3966 - acc: 0.8294 - val_loss: 0.4339 - val_acc: 0.8169\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.41824\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 17s 28ms/step - loss: 0.3687 - acc: 0.8383 - val_loss: 0.3925 - val_acc: 0.8254\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.41824 to 0.39248, saving model to my_checkpoint.ckpt\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 17s 28ms/step - loss: 0.3529 - acc: 0.8451 - val_loss: 0.3966 - val_acc: 0.8228\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.39248\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 17s 28ms/step - loss: 0.3444 - acc: 0.8458 - val_loss: 0.3775 - val_acc: 0.8283\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.39248 to 0.37747, saving model to my_checkpoint.ckpt\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 17s 27ms/step - loss: 0.3329 - acc: 0.8552 - val_loss: 0.4593 - val_acc: 0.7928\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.37747\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 17s 28ms/step - loss: 0.3219 - acc: 0.8595 - val_loss: 0.3731 - val_acc: 0.8245\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.37747 to 0.37315, saving model to my_checkpoint.ckpt\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 17s 28ms/step - loss: 0.3155 - acc: 0.8624 - val_loss: 0.3664 - val_acc: 0.8377\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.37315 to 0.36645, saving model to my_checkpoint.ckpt\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 17s 28ms/step - loss: 0.3054 - acc: 0.8685 - val_loss: 1.0046 - val_acc: 0.6138\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.36645\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}